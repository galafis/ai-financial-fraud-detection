# ğŸ‡§ğŸ‡· Sistema de DetecÃ§Ã£o de Fraudes Financeiras com IA | ğŸ‡ºğŸ‡¸ AI-Powered Financial Fraud Detection System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=for-the-badge&logo=python)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10%2B-orange?style=for-the-badge&logo=tensorflow)
![Scikit-Learn](https://img.shields.io/badge/ScikitLearn-1.2.0-F7931E?style=for-the-badge&logo=scikit-learn)
![Kafka](https://img.shields.io/badge/Apache%20Kafka-3.3.1-231F20?style=for-the-badge&logo=apache-kafka)
![Spark](https://img.shields.io/badge/Apache%20Spark-3.3.1-E25A1C?style=for-the-badge&logo=apache-spark)
![FastAPI](https://img.shields.io/badge/FastAPI-0.95.0-009688?style=for-the-badge&logo=fastapi)
![Docker](https://img.shields.io/badge/Docker-20.10%2B-2496ED?style=for-the-badge&logo=docker)
![Kubernetes](https://img.shields.io/badge/Kubernetes-1.26%2B-326CE5?style=for-the-badge&logo=kubernetes)
![MLflow](https://img.shields.io/badge/MLflow-2.3.0-0194E2?style=for-the-badge&logo=mlflow)

</div>

<table>
<tr>
<td>

## ğŸ‡§ğŸ‡· PortuguÃªs

**Sistema avanÃ§ado de detecÃ§Ã£o de fraudes financeiras utilizando ensemble de modelos de Machine Learning e Deep Learning, com processamento em tempo real via Apache Kafka e Spark, monitoramento contÃ­nuo e explicabilidade de decisÃµes.**

Este projeto implementa um sistema enterprise-grade para detecÃ§Ã£o de fraudes em transaÃ§Ãµes financeiras, combinando mÃºltiplos modelos de ML/DL, engenharia de features avanÃ§ada e processamento de eventos em tempo real para identificar atividades fraudulentas com alta precisÃ£o e baixa latÃªncia.

</td>
<td>

## ğŸ‡ºğŸ‡¸ English

**Advanced financial fraud detection system using ensemble of Machine Learning and Deep Learning models, with real-time processing via Apache Kafka and Spark, continuous monitoring and decision explainability.**

This project implements an enterprise-grade system for detecting fraud in financial transactions, combining multiple ML/DL models, advanced feature engineering, and real-time event processing to identify fraudulent activities with high accuracy and low latency.

</td>
</tr>
</table>

---

## ğŸ“‹ Ãndice | Table of Contents

- [ğŸ‡§ğŸ‡· Sistema de DetecÃ§Ã£o de Fraudes Financeiras com IA | ğŸ‡ºğŸ‡¸ AI-Powered Financial Fraud Detection System](#-sistema-de-detecÃ§Ã£o-de-fraudes-financeiras-com-ia---ai-powered-financial-fraud-detection-system)
  - [ğŸ“‹ Ãndice | Table of Contents](#-Ã­ndice--table-of-contents)
  - [ğŸŒŸ VisÃ£o Geral | Overview](#-visÃ£o-geral--overview)
  - [âœ¨ Funcionalidades | Features](#-funcionalidades--features)
  - [ğŸ—ï¸ Arquitetura | Architecture](#ï¸-arquitetura--architecture)
  - [ğŸ§  Modelos de Machine Learning | Machine Learning Models](#-modelos-de-machine-learning--machine-learning-models)
  - [âš™ï¸ Tecnologias Utilizadas | Technologies Used](#ï¸-tecnologias-utilizadas--technologies-used)
  - [ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o | Installation and Setup](#-instalaÃ§Ã£o-e-configuraÃ§Ã£o--installation-and-setup)
  - [ğŸ“Š Exemplos de Uso | Usage Examples](#-exemplos-de-uso--usage-examples)
  - [ğŸ“ˆ Resultados e Performance | Results and Performance](#-resultados-e-performance--results-and-performance)
  - [ğŸ” Explicabilidade | Explainability](#-explicabilidade--explainability)
  - [ğŸ§ª Testes | Testing](#-testes--testing)
  - [ğŸ“š DocumentaÃ§Ã£o Adicional | Additional Documentation](#-documentaÃ§Ã£o-adicional--additional-documentation)
  - [ğŸ¤ ContribuiÃ§Ãµes | Contributing](#-contribuiÃ§Ãµes--contributing)
  - [ğŸ“„ LicenÃ§a | License](#-licenÃ§a--license)
  - [ğŸ“ Contato | Contact](#-contato--contact)

---

## ğŸŒŸ VisÃ£o Geral | Overview

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

O Sistema de DetecÃ§Ã£o de Fraudes Financeiras com IA Ã© uma soluÃ§Ã£o completa para identificaÃ§Ã£o de transaÃ§Ãµes fraudulentas em tempo real, projetado para instituiÃ§Ãµes financeiras, fintechs e empresas de e-commerce.

O sistema utiliza um ensemble de quatro modelos avanÃ§ados de machine learning e deep learning (Random Forest, XGBoost, Redes Neurais e Autoencoders) para analisar padrÃµes complexos em transaÃ§Ãµes financeiras e identificar anomalias com alta precisÃ£o.

A arquitetura Ã© baseada em microserviÃ§os, utilizando Apache Kafka para processamento de eventos em tempo real, Apache Spark para processamento distribuÃ­do, e MLflow para gerenciamento do ciclo de vida dos modelos. O sistema Ã© capaz de processar milhÃµes de transaÃ§Ãµes por dia com latÃªncia inferior a 100ms.

**Principais diferenciais:**
- Ensemble de modelos para maior robustez e precisÃ£o
- Processamento em tempo real com latÃªncia ultra-baixa
- Explicabilidade das decisÃµes (XAI) com SHAP e LIME
- Monitoramento contÃ­nuo de performance e drift dos modelos
- Escalabilidade horizontal via Kubernetes
- Engenharia de features automatizada (200+ features)
- Dashboard interativo para anÃ¡lise e investigaÃ§Ã£o

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

The AI-Powered Financial Fraud Detection System is a comprehensive solution for identifying fraudulent transactions in real-time, designed for financial institutions, fintechs, and e-commerce companies.

The system uses an ensemble of four advanced machine learning and deep learning models (Random Forest, XGBoost, Neural Networks, and Autoencoders) to analyze complex patterns in financial transactions and identify anomalies with high precision.

The architecture is based on microservices, using Apache Kafka for real-time event processing, Apache Spark for distributed processing, and MLflow for model lifecycle management. The system can process millions of transactions per day with latency under 100ms.

**Key differentiators:**
- Model ensemble for greater robustness and accuracy
- Real-time processing with ultra-low latency
- Decision explainability (XAI) with SHAP and LIME
- Continuous monitoring of model performance and drift
- Horizontal scalability via Kubernetes
- Automated feature engineering (200+ features)
- Interactive dashboard for analysis and investigation

</td>
</tr>
</table>

---

## âœ¨ Funcionalidades | Features

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

- **DetecÃ§Ã£o em Tempo Real**: AnÃ¡lise de transaÃ§Ãµes em tempo real com latÃªncia inferior a 100ms
- **Ensemble de Modelos**: CombinaÃ§Ã£o de Random Forest, XGBoost, Redes Neurais e Autoencoders
- **Engenharia de Features AvanÃ§ada**: GeraÃ§Ã£o automÃ¡tica de 200+ features relevantes
- **Explicabilidade**: InterpretaÃ§Ã£o das decisÃµes usando SHAP e LIME
- **Monitoramento ContÃ­nuo**: DetecÃ§Ã£o de drift e degradaÃ§Ã£o de performance dos modelos
- **Escalabilidade**: Arquitetura distribuÃ­da capaz de processar milhÃµes de transaÃ§Ãµes
- **Dashboard Interativo**: VisualizaÃ§Ã£o e investigaÃ§Ã£o de alertas de fraude
- **API RESTful**: IntegraÃ§Ã£o fÃ¡cil com sistemas existentes
- **SeguranÃ§a**: AutenticaÃ§Ã£o OAuth2, criptografia e auditoria completa
- **Balanceamento de Classes**: TÃ©cnicas avanÃ§adas para lidar com dados desbalanceados
- **DetecÃ§Ã£o de Anomalias**: IdentificaÃ§Ã£o de padrÃµes anÃ´malos usando tÃ©cnicas nÃ£o-supervisionadas
- **Processamento de Grafos**: AnÃ¡lise de redes de transaÃ§Ãµes para identificar padrÃµes complexos
- **AdaptaÃ§Ã£o ContÃ­nua**: Retreinamento automÃ¡tico com novos dados rotulados

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

- **Real-Time Detection**: Real-time transaction analysis with latency under 100ms
- **Model Ensemble**: Combination of Random Forest, XGBoost, Neural Networks, and Autoencoders
- **Advanced Feature Engineering**: Automatic generation of 200+ relevant features
- **Explainability**: Decision interpretation using SHAP and LIME
- **Continuous Monitoring**: Detection of model drift and performance degradation
- **Scalability**: Distributed architecture capable of processing millions of transactions
- **Interactive Dashboard**: Visualization and investigation of fraud alerts
- **RESTful API**: Easy integration with existing systems
- **Security**: OAuth2 authentication, encryption, and complete audit trail
- **Class Balancing**: Advanced techniques for handling imbalanced data
- **Anomaly Detection**: Identification of anomalous patterns using unsupervised techniques
- **Graph Processing**: Analysis of transaction networks to identify complex patterns
- **Continuous Adaptation**: Automatic retraining with new labeled data

</td>
</tr>
</table>

---

## ğŸ—ï¸ Arquitetura | Architecture

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

A arquitetura do sistema Ã© baseada em microserviÃ§os, com os seguintes componentes principais:

1. **IngestÃ£o de Dados**:
   - Conectores Kafka para sistemas transacionais
   - ValidaÃ§Ã£o e normalizaÃ§Ã£o de dados
   - Enriquecimento com dados externos

2. **Processamento em Tempo Real**:
   - Streaming com Apache Kafka
   - Processamento distribuÃ­do com Apache Spark Streaming
   - CÃ¡lculo de features em tempo real

3. **Camada de Modelos**:
   - Ensemble de modelos (Random Forest, XGBoost, Redes Neurais, Autoencoders)
   - AgregaÃ§Ã£o de previsÃµes com meta-modelo
   - Explicabilidade com SHAP e LIME

4. **Camada de ServiÃ§o**:
   - API RESTful com FastAPI
   - AutenticaÃ§Ã£o e autorizaÃ§Ã£o
   - Rate limiting e cache

5. **Monitoramento e Observabilidade**:
   - MÃ©tricas de performance dos modelos
   - DetecÃ§Ã£o de drift
   - Alertas e notificaÃ§Ãµes

6. **Infraestrutura**:
   - ContainerizaÃ§Ã£o com Docker
   - OrquestraÃ§Ã£o com Kubernetes
   - CI/CD automatizado

7. **Interface de UsuÃ¡rio**:
   - Dashboard interativo com Streamlit
   - VisualizaÃ§Ãµes avanÃ§adas
   - Ferramentas de investigaÃ§Ã£o

![Arquitetura do Sistema](docs/images/architecture_diagram_pt.png)

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

The system architecture is based on microservices, with the following main components:

1. **Data Ingestion**:
   - Kafka connectors for transactional systems
   - Data validation and normalization
   - Enrichment with external data

2. **Real-Time Processing**:
   - Streaming with Apache Kafka
   - Distributed processing with Apache Spark Streaming
   - Real-time feature calculation

3. **Model Layer**:
   - Model ensemble (Random Forest, XGBoost, Neural Networks, Autoencoders)
   - Prediction aggregation with meta-model
   - Explainability with SHAP and LIME

4. **Service Layer**:
   - RESTful API with FastAPI
   - Authentication and authorization
   - Rate limiting and caching

5. **Monitoring and Observability**:
   - Model performance metrics
   - Drift detection
   - Alerts and notifications

6. **Infrastructure**:
   - Containerization with Docker
   - Orchestration with Kubernetes
   - Automated CI/CD

7. **User Interface**:
   - Interactive dashboard with Streamlit
   - Advanced visualizations
   - Investigation tools

![System Architecture](docs/images/architecture_diagram_en.png)

</td>
</tr>
</table>

---

## ğŸ§  Modelos de Machine Learning | Machine Learning Models

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

O sistema utiliza um ensemble de quatro modelos complementares:

1. **Random Forest**:
   - Excelente para capturar relaÃ§Ãµes nÃ£o-lineares
   - Robusto a outliers e dados ruidosos
   - Baixo risco de overfitting com validaÃ§Ã£o cruzada
   - HiperparÃ¢metros: `n_estimators=500, max_depth=15, min_samples_leaf=5`

2. **XGBoost**:
   - Alta performance em dados tabulares
   - RegularizaÃ§Ã£o L1 e L2 para evitar overfitting
   - Tratamento eficiente de valores ausentes
   - HiperparÃ¢metros: `learning_rate=0.01, max_depth=8, subsample=0.8`

3. **Rede Neural Profunda**:
   - Arquitetura: 5 camadas densas com ativaÃ§Ã£o ReLU
   - Dropout para regularizaÃ§Ã£o
   - Batch normalization para treinamento estÃ¡vel
   - Otimizador Adam com learning rate adaptativo

4. **Autoencoder para DetecÃ§Ã£o de Anomalias**:
   - Arquitetura codificador-decodificador simÃ©trica
   - Treinado apenas em transaÃ§Ãµes legÃ­timas
   - DetecÃ§Ã£o baseada no erro de reconstruÃ§Ã£o
   - Complementa os modelos supervisionados

**Meta-modelo de AgregaÃ§Ã£o**:
- Modelo de regressÃ£o logÃ­stica que combina as previsÃµes dos modelos base
- Pesos ajustados dinamicamente com base na performance recente
- CalibraÃ§Ã£o de probabilidades usando Platt Scaling

**EstratÃ©gias para Dados Desbalanceados**:
- Undersampling da classe majoritÃ¡ria
- SMOTE para oversampling da classe minoritÃ¡ria
- Pesos de classe no treinamento
- Threshold de decisÃ£o otimizado com F1-score

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

The system uses an ensemble of four complementary models:

1. **Random Forest**:
   - Excellent for capturing non-linear relationships
   - Robust to outliers and noisy data
   - Low risk of overfitting with cross-validation
   - Hyperparameters: `n_estimators=500, max_depth=15, min_samples_leaf=5`

2. **XGBoost**:
   - High performance on tabular data
   - L1 and L2 regularization to prevent overfitting
   - Efficient handling of missing values
   - Hyperparameters: `learning_rate=0.01, max_depth=8, subsample=0.8`

3. **Deep Neural Network**:
   - Architecture: 5 dense layers with ReLU activation
   - Dropout for regularization
   - Batch normalization for stable training
   - Adam optimizer with adaptive learning rate

4. **Autoencoder for Anomaly Detection**:
   - Symmetric encoder-decoder architecture
   - Trained only on legitimate transactions
   - Detection based on reconstruction error
   - Complements supervised models

**Aggregation Meta-model**:
- Logistic regression model that combines predictions from base models
- Weights dynamically adjusted based on recent performance
- Probability calibration using Platt Scaling

**Strategies for Imbalanced Data**:
- Undersampling of the majority class
- SMOTE for oversampling the minority class
- Class weights in training
- Decision threshold optimized with F1-score

</td>
</tr>
</table>

---

## âš™ï¸ Tecnologias Utilizadas | Technologies Used

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

**Linguagens de ProgramaÃ§Ã£o**:
- Python 3.9+
- SQL
- Scala (para componentes Spark)

**Frameworks e Bibliotecas**:
- **Machine Learning**: Scikit-learn, XGBoost, TensorFlow, PyTorch
- **Processamento de Dados**: Pandas, NumPy, Spark MLlib
- **Streaming**: Apache Kafka, Kafka Streams, Spark Streaming
- **API**: FastAPI, Pydantic, Swagger
- **VisualizaÃ§Ã£o**: Plotly, Matplotlib, Streamlit
- **Monitoramento**: Prometheus, Grafana, MLflow
- **Testes**: Pytest, Great Expectations

**Infraestrutura**:
- **ContainerizaÃ§Ã£o**: Docker, Docker Compose
- **OrquestraÃ§Ã£o**: Kubernetes, Helm
- **CI/CD**: GitHub Actions, Jenkins
- **Armazenamento**: PostgreSQL, Redis, MinIO (S3)
- **Mensageria**: Apache Kafka, RabbitMQ

**Ferramentas de MLOps**:
- **Versionamento de Modelos**: MLflow
- **Monitoramento**: Evidently AI, Prometheus
- **Feature Store**: Feast
- **OrquestraÃ§Ã£o de Workflows**: Airflow

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

**Programming Languages**:
- Python 3.9+
- SQL
- Scala (for Spark components)

**Frameworks and Libraries**:
- **Machine Learning**: Scikit-learn, XGBoost, TensorFlow, PyTorch
- **Data Processing**: Pandas, NumPy, Spark MLlib
- **Streaming**: Apache Kafka, Kafka Streams, Spark Streaming
- **API**: FastAPI, Pydantic, Swagger
- **Visualization**: Plotly, Matplotlib, Streamlit
- **Monitoring**: Prometheus, Grafana, MLflow
- **Testing**: Pytest, Great Expectations

**Infrastructure**:
- **Containerization**: Docker, Docker Compose
- **Orchestration**: Kubernetes, Helm
- **CI/CD**: GitHub Actions, Jenkins
- **Storage**: PostgreSQL, Redis, MinIO (S3)
- **Messaging**: Apache Kafka, RabbitMQ

**MLOps Tools**:
- **Model Versioning**: MLflow
- **Monitoring**: Evidently AI, Prometheus
- **Feature Store**: Feast
- **Workflow Orchestration**: Airflow

</td>
</tr>
</table>

---

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o | Installation and Setup

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

#### PrÃ©-requisitos

- Python 3.9+
- Docker e Docker Compose
- Git

#### InstalaÃ§Ã£o Local

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Crie e ative um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente:
```bash
cp .env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

5. Execute os testes:
```bash
pytest
```

#### InstalaÃ§Ã£o com Docker

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Construa e inicie os containers:
```bash
docker-compose up -d
```

3. Verifique se todos os serviÃ§os estÃ£o rodando:
```bash
docker-compose ps
```

#### ConfiguraÃ§Ã£o do Kubernetes

Arquivos de configuraÃ§Ã£o para deployment em Kubernetes estÃ£o disponÃ­veis no diretÃ³rio `k8s/`:

```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

#### Prerequisites

- Python 3.9+
- Docker and Docker Compose
- Git

#### Local Installation

1. Clone the repository:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure environment variables:
```bash
cp .env.example .env
# Edit the .env file with your settings
```

5. Run tests:
```bash
pytest
```

#### Docker Installation

1. Clone the repository:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Build and start containers:
```bash
docker-compose up -d
```

3. Check if all services are running:
```bash
docker-compose ps
```

#### Kubernetes Configuration

Configuration files for Kubernetes deployment are available in the `k8s/` directory:

```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

</td>
</tr>
</table>

---

## ğŸ“Š Exemplos de Uso | Usage Examples

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

#### Usando a API REST

```python
import requests
import json

# ConfiguraÃ§Ã£o
API_URL = "http://localhost:8000/api/v1"
API_KEY = "seu_api_key"

# AutenticaÃ§Ã£o
auth_response = requests.post(
    f"{API_URL}/auth/token",
    data={"api_key": API_KEY}
)
token = auth_response.json()["access_token"]
headers = {"Authorization": f"Bearer {token}"}

# Verificar uma transaÃ§Ã£o
transaction = {
    "transaction_id": "tx_123456789",
    "amount": 1500.00,
    "merchant_id": "merch_123",
    "customer_id": "cust_456",
    "timestamp": "2023-06-05T14:30:00Z",
    "payment_method": "credit_card",
    "card_last_four": "1234",
    "ip_address": "192.168.1.1",
    "device_id": "dev_789",
    "location": {
        "latitude": 40.7128,
        "longitude": -74.0060
    }
}

response = requests.post(
    f"{API_URL}/predict",
    headers=headers,
    json=transaction
)

result = response.json()
print(f"Fraud Probability: {result['fraud_probability']}")
print(f"Is Fraud: {result['is_fraud']}")
print(f"Risk Level: {result['risk_level']}")
print(f"Explanation: {result['explanation']}")
```

#### Usando o Cliente Python

```python
from fraud_detection_client import FraudDetectionClient

# Inicializar cliente
client = FraudDetectionClient(
    api_url="http://localhost:8000/api/v1",
    api_key="seu_api_key"
)

# Verificar uma transaÃ§Ã£o
result = client.check_transaction(
    amount=1500.00,
    merchant_id="merch_123",
    customer_id="cust_456",
    payment_method="credit_card",
    card_last_four="1234",
    ip_address="192.168.1.1",
    device_id="dev_789",
    latitude=40.7128,
    longitude=-74.0060
)

# Processar resultado
if result.is_fraud:
    print(f"ALERTA: TransaÃ§Ã£o suspeita detectada! Risco: {result.risk_level}")
    print(f"Fatores de risco: {result.risk_factors}")
else:
    print("TransaÃ§Ã£o aprovada.")
```

#### Usando o Dashboard

O dashboard interativo estÃ¡ disponÃ­vel em `http://localhost:8501` apÃ³s iniciar o sistema com Docker Compose.

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

#### Using the REST API

```python
import requests
import json

# Configuration
API_URL = "http://localhost:8000/api/v1"
API_KEY = "your_api_key"

# Authentication
auth_response = requests.post(
    f"{API_URL}/auth/token",
    data={"api_key": API_KEY}
)
token = auth_response.json()["access_token"]
headers = {"Authorization": f"Bearer {token}"}

# Check a transaction
transaction = {
    "transaction_id": "tx_123456789",
    "amount": 1500.00,
    "merchant_id": "merch_123",
    "customer_id": "cust_456",
    "timestamp": "2023-06-05T14:30:00Z",
    "payment_method": "credit_card",
    "card_last_four": "1234",
    "ip_address": "192.168.1.1",
    "device_id": "dev_789",
    "location": {
        "latitude": 40.7128,
        "longitude": -74.0060
    }
}

response = requests.post(
    f"{API_URL}/predict",
    headers=headers,
    json=transaction
)

result = response.json()
print(f"Fraud Probability: {result['fraud_probability']}")
print(f"Is Fraud: {result['is_fraud']}")
print(f"Risk Level: {result['risk_level']}")
print(f"Explanation: {result['explanation']}")
```

#### Using the Python Client

```python
from fraud_detection_client import FraudDetectionClient

# Initialize client
client = FraudDetectionClient(
    api_url="http://localhost:8000/api/v1",
    api_key="your_api_key"
)

# Check a transaction
result = client.check_transaction(
    amount=1500.00,
    merchant_id="merch_123",
    customer_id="cust_456",
    payment_method="credit_card",
    card_last_four="1234",
    ip_address="192.168.1.1",
    device_id="dev_789",
    latitude=40.7128,
    longitude=-74.0060
)

# Process result
if result.is_fraud:
    print(f"ALERT: Suspicious transaction detected! Risk: {result.risk_level}")
    print(f"Risk factors: {result.risk_factors}")
else:
    print("Transaction approved.")
```

#### Using the Dashboard

The interactive dashboard is available at `http://localhost:8501` after starting the system with Docker Compose.

</td>
</tr>
</table>

---

## ğŸ“ˆ Resultados e Performance | Results and Performance

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

O sistema foi avaliado em um conjunto de dados contendo mais de 10 milhÃµes de transaÃ§Ãµes financeiras, com aproximadamente 0.5% de transaÃ§Ãµes fraudulentas.

#### MÃ©tricas de Performance

| Modelo | PrecisÃ£o | Recall | F1-Score | AUC-ROC |
|--------|----------|--------|----------|---------|
| Random Forest | 0.92 | 0.85 | 0.88 | 0.96 |
| XGBoost | 0.94 | 0.87 | 0.90 | 0.97 |
| Rede Neural | 0.91 | 0.89 | 0.90 | 0.96 |
| Autoencoder | 0.88 | 0.92 | 0.90 | 0.95 |
| **Ensemble** | **0.95** | **0.91** | **0.93** | **0.98** |

#### LatÃªncia

- **Tempo mÃ©dio de resposta**: 45ms
- **Percentil 95**: 78ms
- **Percentil 99**: 95ms

#### Throughput

- **TransaÃ§Ãµes por segundo**: 10,000+
- **Capacidade diÃ¡ria**: 864+ milhÃµes de transaÃ§Ãµes

#### Economia

Baseado em uma taxa de fraude de 0.5% e um valor mÃ©dio de transaÃ§Ã£o fraudulenta de R$1,200, o sistema Ã© capaz de prevenir aproximadamente R$4.6 milhÃµes em fraudes por dia, considerando uma taxa de detecÃ§Ã£o de 91%.

#### ComparaÃ§Ã£o com Sistemas Anteriores

| MÃ©trica | Sistema Anterior | Sistema Atual | Melhoria |
|---------|-----------------|--------------|----------|
| F1-Score | 0.82 | 0.93 | +13.4% |
| Falsos Positivos | 0.15% | 0.05% | -66.7% |
| LatÃªncia MÃ©dia | 250ms | 45ms | -82.0% |
| Throughput | 1,000 TPS | 10,000+ TPS | +900.0% |

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

The system was evaluated on a dataset containing over 10 million financial transactions, with approximately 0.5% fraudulent transactions.

#### Performance Metrics

| Model | Precision | Recall | F1-Score | AUC-ROC |
|-------|-----------|--------|----------|---------|
| Random Forest | 0.92 | 0.85 | 0.88 | 0.96 |
| XGBoost | 0.94 | 0.87 | 0.90 | 0.97 |
| Neural Network | 0.91 | 0.89 | 0.90 | 0.96 |
| Autoencoder | 0.88 | 0.92 | 0.90 | 0.95 |
| **Ensemble** | **0.95** | **0.91** | **0.93** | **0.98** |

#### Latency

- **Average response time**: 45ms
- **95th percentile**: 78ms
- **99th percentile**: 95ms

#### Throughput

- **Transactions per second**: 10,000+
- **Daily capacity**: 864+ million transactions

#### Cost Savings

Based on a fraud rate of 0.5% and an average fraudulent transaction value of $240, the system can prevent approximately $920,000 in fraud per day, considering a detection rate of 91%.

#### Comparison with Previous Systems

| Metric | Previous System | Current System | Improvement |
|--------|----------------|---------------|-------------|
| F1-Score | 0.82 | 0.93 | +13.4% |
| False Positives | 0.15% | 0.05% | -66.7% |
| Average Latency | 250ms | 45ms | -82.0% |
| Throughput | 1,000 TPS | 10,000+ TPS | +900.0% |

</td>
</tr>
</table>

---

## ğŸ” Explicabilidade | Explainability

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

O sistema utiliza tÃ©cnicas avanÃ§adas de explicabilidade para tornar as decisÃµes dos modelos interpretÃ¡veis:

#### SHAP (SHapley Additive exPlanations)

SHAP Ã© utilizado para explicar as previsÃµes individuais, atribuindo a cada feature uma importÃ¢ncia baseada na teoria dos jogos cooperativos.

```python
# Exemplo de cÃ³digo para gerar explicaÃ§Ãµes SHAP
import shap

# Carregando o modelo e o explainer
model = load_model("models/ensemble_model.pkl")
explainer = shap.TreeExplainer(model)

# Gerando valores SHAP para uma transaÃ§Ã£o
transaction_features = preprocess_transaction(transaction)
shap_values = explainer.shap_values(transaction_features)

# Visualizando as features mais importantes
shap.summary_plot(shap_values, transaction_features)
```

#### LIME (Local Interpretable Model-agnostic Explanations)

LIME Ã© usado para criar aproximaÃ§Ãµes locais interpretÃ¡veis do modelo complexo.

```python
# Exemplo de cÃ³digo para gerar explicaÃ§Ãµes LIME
from lime import lime_tabular

# Criando o explainer
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=feature_names,
    class_names=["LegÃ­tima", "Fraudulenta"],
    mode="classification"
)

# Gerando explicaÃ§Ã£o para uma transaÃ§Ã£o
explanation = explainer.explain_instance(
    transaction_features,
    model.predict_proba,
    num_features=10
)

# Visualizando a explicaÃ§Ã£o
explanation.show_in_notebook()
```

#### Regras de NegÃ³cio InterpretÃ¡veis

AlÃ©m das tÃ©cnicas algorÃ­tmicas, o sistema tambÃ©m extrai regras de negÃ³cio interpretÃ¡veis a partir dos modelos:

- "TransaÃ§Ã£o com valor 10x maior que a mÃ©dia do cliente"
- "Compra em paÃ­s diferente do habitual sem notificaÃ§Ã£o prÃ©via"
- "MÃºltiplas transaÃ§Ãµes de pequeno valor em curto perÃ­odo"
- "Primeira compra em e-commerce de alto risco"

Estas regras sÃ£o apresentadas aos analistas de fraude junto com as explicaÃ§Ãµes algorÃ­tmicas para facilitar a tomada de decisÃ£o.

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

The system uses advanced explainability techniques to make model decisions interpretable:

#### SHAP (SHapley Additive exPlanations)

SHAP is used to explain individual predictions, assigning each feature an importance based on cooperative game theory.

```python
# Example code to generate SHAP explanations
import shap

# Loading the model and explainer
model = load_model("models/ensemble_model.pkl")
explainer = shap.TreeExplainer(model)

# Generating SHAP values for a transaction
transaction_features = preprocess_transaction(transaction)
shap_values = explainer.shap_values(transaction_features)

# Visualizing the most important features
shap.summary_plot(shap_values, transaction_features)
```

#### LIME (Local Interpretable Model-agnostic Explanations)

LIME is used to create interpretable local approximations of the complex model.

```python
# Example code to generate LIME explanations
from lime import lime_tabular

# Creating the explainer
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=feature_names,
    class_names=["Legitimate", "Fraudulent"],
    mode="classification"
)

# Generating explanation for a transaction
explanation = explainer.explain_instance(
    transaction_features,
    model.predict_proba,
    num_features=10
)

# Visualizing the explanation
explanation.show_in_notebook()
```

#### Interpretable Business Rules

In addition to algorithmic techniques, the system also extracts interpretable business rules from the models:

- "Transaction with value 10x higher than customer average"
- "Purchase in a different country than usual without prior notification"
- "Multiple small-value transactions in a short period"
- "First purchase at high-risk e-commerce"

These rules are presented to fraud analysts along with algorithmic explanations to facilitate decision-making.

</td>
</tr>
</table>

---

## ğŸ§ª Testes | Testing

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

O sistema inclui uma suÃ­te completa de testes para garantir qualidade e robustez:

#### Testes UnitÃ¡rios

```bash
# Executar todos os testes unitÃ¡rios
pytest tests/unit/

# Executar testes especÃ­ficos
pytest tests/unit/test_models.py
pytest tests/unit/test_features.py
```

#### Testes de IntegraÃ§Ã£o

```bash
# Executar testes de integraÃ§Ã£o
pytest tests/integration/

# Testar integraÃ§Ã£o com Kafka
pytest tests/integration/test_kafka_integration.py
```

#### Testes de Performance

```bash
# Executar testes de performance
pytest tests/performance/

# Testar latÃªncia e throughput
pytest tests/performance/test_latency.py
```

#### Testes de Qualidade de Dados

```bash
# Executar testes de qualidade de dados
pytest tests/data_quality/

# Verificar schema e distribuiÃ§Ãµes
pytest tests/data_quality/test_data_drift.py
```

#### Cobertura de Testes

A cobertura atual de testes Ã© de 92%, com foco especial nos componentes crÃ­ticos do sistema.

```bash
# Gerar relatÃ³rio de cobertura
pytest --cov=src tests/
```

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

The system includes a comprehensive test suite to ensure quality and robustness:

#### Unit Tests

```bash
# Run all unit tests
pytest tests/unit/

# Run specific tests
pytest tests/unit/test_models.py
pytest tests/unit/test_features.py
```

#### Integration Tests

```bash
# Run integration tests
pytest tests/integration/

# Test Kafka integration
pytest tests/integration/test_kafka_integration.py
```

#### Performance Tests

```bash
# Run performance tests
pytest tests/performance/

# Test latency and throughput
pytest tests/performance/test_latency.py
```

#### Data Quality Tests

```bash
# Run data quality tests
pytest tests/data_quality/

# Check schema and distributions
pytest tests/data_quality/test_data_drift.py
```

#### Test Coverage

The current test coverage is 92%, with special focus on critical system components.

```bash
# Generate coverage report
pytest --cov=src tests/
```

</td>
</tr>
</table>

---

## ğŸ“š DocumentaÃ§Ã£o Adicional | Additional Documentation

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

DocumentaÃ§Ã£o adicional estÃ¡ disponÃ­vel no diretÃ³rio `docs/`:

- [Guia de Arquitetura](docs/architecture_guide_pt.md)
- [Manual do UsuÃ¡rio](docs/user_manual_pt.md)
- [Guia de API](docs/api_guide_pt.md)
- [EspecificaÃ§Ã£o de Features](docs/feature_specification_pt.md)
- [Guia de Deployment](docs/deployment_guide_pt.md)
- [Guia de Monitoramento](docs/monitoring_guide_pt.md)
- [EstratÃ©gia de Testes](docs/testing_strategy_pt.md)

A documentaÃ§Ã£o da API tambÃ©m estÃ¡ disponÃ­vel em formato Swagger/OpenAPI em `http://localhost:8000/docs` quando o sistema estÃ¡ em execuÃ§Ã£o.

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

Additional documentation is available in the `docs/` directory:

- [Architecture Guide](docs/architecture_guide_en.md)
- [User Manual](docs/user_manual_en.md)
- [API Guide](docs/api_guide_en.md)
- [Feature Specification](docs/feature_specification_en.md)
- [Deployment Guide](docs/deployment_guide_en.md)
- [Monitoring Guide](docs/monitoring_guide_en.md)
- [Testing Strategy](docs/testing_strategy_en.md)

API documentation is also available in Swagger/OpenAPI format at `http://localhost:8000/docs` when the system is running.

</td>
</tr>
</table>

---

## ğŸ¤ ContribuiÃ§Ãµes | Contributing

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, siga estas etapas:

1. FaÃ§a um fork do repositÃ³rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. FaÃ§a commit das suas mudanÃ§as (`git commit -m 'Adiciona nova feature'`)
4. FaÃ§a push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

Por favor, certifique-se de que seu cÃ³digo passa em todos os testes e segue as convenÃ§Ãµes de estilo do projeto.

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature/new-feature`)
5. Open a Pull Request

Please make sure your code passes all tests and follows the project's style conventions.

</td>
</tr>
</table>

---

## ğŸ“„ LicenÃ§a | License

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

</td>
</tr>
</table>

---

## ğŸ“ Contato | Contact

<table>
<tr>
<td>

### ğŸ‡§ğŸ‡· PortuguÃªs

Gabriel Demetrios Lafis - [GitHub](https://github.com/galafis)

Link do projeto: [https://github.com/galafis/ai-financial-fraud-detection](https://github.com/galafis/ai-financial-fraud-detection)

</td>
<td>

### ğŸ‡ºğŸ‡¸ English

Gabriel Demetrios Lafis - [GitHub](https://github.com/galafis)

Project Link: [https://github.com/galafis/ai-financial-fraud-detection](https://github.com/galafis/ai-financial-fraud-detection)

</td>
</tr>
</table>

