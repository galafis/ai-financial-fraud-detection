# 🇧🇷 Sistema de Detecção de Fraudes Financeiras com IA | 🇺🇸 AI-Powered Financial Fraud Detection System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=for-the-badge&logo=python)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10%2B-orange?style=for-the-badge&logo=tensorflow)
![Scikit-Learn](https://img.shields.io/badge/ScikitLearn-1.2.0-F7931E?style=for-the-badge&logo=scikit-learn)
![Kafka](https://img.shields.io/badge/Apache%20Kafka-3.3.1-231F20?style=for-the-badge&logo=apache-kafka)
![Spark](https://img.shields.io/badge/Apache%20Spark-3.3.1-E25A1C?style=for-the-badge&logo=apache-spark)
![FastAPI](https://img.shields.io/badge/FastAPI-0.95.0-009688?style=for-the-badge&logo=fastapi)
![Docker](https://img.shields.io/badge/Docker-20.10%2B-2496ED?style=for-the-badge&logo=docker)
![Kubernetes](https://img.shields.io/badge/Kubernetes-1.26%2B-326CE5?style=for-the-badge&logo=kubernetes)
![MLflow](https://img.shields.io/badge/MLflow-2.3.0-0194E2?style=for-the-badge&logo=mlflow)

</div>

<table>
<tr>
<td>

## 🇧🇷 Português

**Sistema avançado de detecção de fraudes financeiras utilizando ensemble de modelos de Machine Learning e Deep Learning, com processamento em tempo real via Apache Kafka e Spark, monitoramento contínuo e explicabilidade de decisões.**

Este projeto implementa um sistema enterprise-grade para detecção de fraudes em transações financeiras, combinando múltiplos modelos de ML/DL, engenharia de features avançada e processamento de eventos em tempo real para identificar atividades fraudulentas com alta precisão e baixa latência.

</td>
<td>

## 🇺🇸 English

**Advanced financial fraud detection system using ensemble of Machine Learning and Deep Learning models, with real-time processing via Apache Kafka and Spark, continuous monitoring and decision explainability.**

This project implements an enterprise-grade system for detecting fraud in financial transactions, combining multiple ML/DL models, advanced feature engineering, and real-time event processing to identify fraudulent activities with high accuracy and low latency.

</td>
</tr>
</table>

---

## 📋 Índice | Table of Contents

- [🇧🇷 Sistema de Detecção de Fraudes Financeiras com IA | 🇺🇸 AI-Powered Financial Fraud Detection System](#-sistema-de-detecção-de-fraudes-financeiras-com-ia---ai-powered-financial-fraud-detection-system)
  - [📋 Índice | Table of Contents](#-índice--table-of-contents)
  - [🌟 Visão Geral | Overview](#-visão-geral--overview)
  - [✨ Funcionalidades | Features](#-funcionalidades--features)
  - [🏗️ Arquitetura | Architecture](#️-arquitetura--architecture)
  - [🧠 Modelos de Machine Learning | Machine Learning Models](#-modelos-de-machine-learning--machine-learning-models)
  - [⚙️ Tecnologias Utilizadas | Technologies Used](#️-tecnologias-utilizadas--technologies-used)
  - [🚀 Instalação e Configuração | Installation and Setup](#-instalação-e-configuração--installation-and-setup)
  - [📊 Exemplos de Uso | Usage Examples](#-exemplos-de-uso--usage-examples)
  - [📈 Resultados e Performance | Results and Performance](#-resultados-e-performance--results-and-performance)
  - [🔍 Explicabilidade | Explainability](#-explicabilidade--explainability)
  - [🧪 Testes | Testing](#-testes--testing)
  - [📚 Documentação Adicional | Additional Documentation](#-documentação-adicional--additional-documentation)
  - [🤝 Contribuições | Contributing](#-contribuições--contributing)
  - [📄 Licença | License](#-licença--license)
  - [📞 Contato | Contact](#-contato--contact)

---

## 🌟 Visão Geral | Overview

<table>
<tr>
<td>

### 🇧🇷 Português

O Sistema de Detecção de Fraudes Financeiras com IA é uma solução completa para identificação de transações fraudulentas em tempo real, projetado para instituições financeiras, fintechs e empresas de e-commerce.

O sistema utiliza um ensemble de quatro modelos avançados de machine learning e deep learning (Random Forest, XGBoost, Redes Neurais e Autoencoders) para analisar padrões complexos em transações financeiras e identificar anomalias com alta precisão.

A arquitetura é baseada em microserviços, utilizando Apache Kafka para processamento de eventos em tempo real, Apache Spark para processamento distribuído, e MLflow para gerenciamento do ciclo de vida dos modelos. O sistema é capaz de processar milhões de transações por dia com latência inferior a 100ms.

**Principais diferenciais:**
- Ensemble de modelos para maior robustez e precisão
- Processamento em tempo real com latência ultra-baixa
- Explicabilidade das decisões (XAI) com SHAP e LIME
- Monitoramento contínuo de performance e drift dos modelos
- Escalabilidade horizontal via Kubernetes
- Engenharia de features automatizada (200+ features)
- Dashboard interativo para análise e investigação

</td>
<td>

### 🇺🇸 English

The AI-Powered Financial Fraud Detection System is a comprehensive solution for identifying fraudulent transactions in real-time, designed for financial institutions, fintechs, and e-commerce companies.

The system uses an ensemble of four advanced machine learning and deep learning models (Random Forest, XGBoost, Neural Networks, and Autoencoders) to analyze complex patterns in financial transactions and identify anomalies with high precision.

The architecture is based on microservices, using Apache Kafka for real-time event processing, Apache Spark for distributed processing, and MLflow for model lifecycle management. The system can process millions of transactions per day with latency under 100ms.

**Key differentiators:**
- Model ensemble for greater robustness and accuracy
- Real-time processing with ultra-low latency
- Decision explainability (XAI) with SHAP and LIME
- Continuous monitoring of model performance and drift
- Horizontal scalability via Kubernetes
- Automated feature engineering (200+ features)
- Interactive dashboard for analysis and investigation

</td>
</tr>
</table>

---

## ✨ Funcionalidades | Features

<table>
<tr>
<td>

### 🇧🇷 Português

- **Detecção em Tempo Real**: Análise de transações em tempo real com latência inferior a 100ms
- **Ensemble de Modelos**: Combinação de Random Forest, XGBoost, Redes Neurais e Autoencoders
- **Engenharia de Features Avançada**: Geração automática de 200+ features relevantes
- **Explicabilidade**: Interpretação das decisões usando SHAP e LIME
- **Monitoramento Contínuo**: Detecção de drift e degradação de performance dos modelos
- **Escalabilidade**: Arquitetura distribuída capaz de processar milhões de transações
- **Dashboard Interativo**: Visualização e investigação de alertas de fraude
- **API RESTful**: Integração fácil com sistemas existentes
- **Segurança**: Autenticação OAuth2, criptografia e auditoria completa
- **Balanceamento de Classes**: Técnicas avançadas para lidar com dados desbalanceados
- **Detecção de Anomalias**: Identificação de padrões anômalos usando técnicas não-supervisionadas
- **Processamento de Grafos**: Análise de redes de transações para identificar padrões complexos
- **Adaptação Contínua**: Retreinamento automático com novos dados rotulados

</td>
<td>

### 🇺🇸 English

- **Real-Time Detection**: Real-time transaction analysis with latency under 100ms
- **Model Ensemble**: Combination of Random Forest, XGBoost, Neural Networks, and Autoencoders
- **Advanced Feature Engineering**: Automatic generation of 200+ relevant features
- **Explainability**: Decision interpretation using SHAP and LIME
- **Continuous Monitoring**: Detection of model drift and performance degradation
- **Scalability**: Distributed architecture capable of processing millions of transactions
- **Interactive Dashboard**: Visualization and investigation of fraud alerts
- **RESTful API**: Easy integration with existing systems
- **Security**: OAuth2 authentication, encryption, and complete audit trail
- **Class Balancing**: Advanced techniques for handling imbalanced data
- **Anomaly Detection**: Identification of anomalous patterns using unsupervised techniques
- **Graph Processing**: Analysis of transaction networks to identify complex patterns
- **Continuous Adaptation**: Automatic retraining with new labeled data

</td>
</tr>
</table>

---

## 🏗️ Arquitetura | Architecture

<table>
<tr>
<td>

### 🇧🇷 Português

A arquitetura do sistema é baseada em microserviços, com os seguintes componentes principais:

1. **Ingestão de Dados**:
   - Conectores Kafka para sistemas transacionais
   - Validação e normalização de dados
   - Enriquecimento com dados externos

2. **Processamento em Tempo Real**:
   - Streaming com Apache Kafka
   - Processamento distribuído com Apache Spark Streaming
   - Cálculo de features em tempo real

3. **Camada de Modelos**:
   - Ensemble de modelos (Random Forest, XGBoost, Redes Neurais, Autoencoders)
   - Agregação de previsões com meta-modelo
   - Explicabilidade com SHAP e LIME

4. **Camada de Serviço**:
   - API RESTful com FastAPI
   - Autenticação e autorização
   - Rate limiting e cache

5. **Monitoramento e Observabilidade**:
   - Métricas de performance dos modelos
   - Detecção de drift
   - Alertas e notificações

6. **Infraestrutura**:
   - Containerização com Docker
   - Orquestração com Kubernetes
   - CI/CD automatizado

7. **Interface de Usuário**:
   - Dashboard interativo com Streamlit
   - Visualizações avançadas
   - Ferramentas de investigação

![Arquitetura do Sistema](docs/images/architecture_diagram_pt.png)

</td>
<td>

### 🇺🇸 English

The system architecture is based on microservices, with the following main components:

1. **Data Ingestion**:
   - Kafka connectors for transactional systems
   - Data validation and normalization
   - Enrichment with external data

2. **Real-Time Processing**:
   - Streaming with Apache Kafka
   - Distributed processing with Apache Spark Streaming
   - Real-time feature calculation

3. **Model Layer**:
   - Model ensemble (Random Forest, XGBoost, Neural Networks, Autoencoders)
   - Prediction aggregation with meta-model
   - Explainability with SHAP and LIME

4. **Service Layer**:
   - RESTful API with FastAPI
   - Authentication and authorization
   - Rate limiting and caching

5. **Monitoring and Observability**:
   - Model performance metrics
   - Drift detection
   - Alerts and notifications

6. **Infrastructure**:
   - Containerization with Docker
   - Orchestration with Kubernetes
   - Automated CI/CD

7. **User Interface**:
   - Interactive dashboard with Streamlit
   - Advanced visualizations
   - Investigation tools

![System Architecture](docs/images/architecture_diagram_en.png)

</td>
</tr>
</table>

---

## 🧠 Modelos de Machine Learning | Machine Learning Models

<table>
<tr>
<td>

### 🇧🇷 Português

O sistema utiliza um ensemble de quatro modelos complementares:

1. **Random Forest**:
   - Excelente para capturar relações não-lineares
   - Robusto a outliers e dados ruidosos
   - Baixo risco de overfitting com validação cruzada
   - Hiperparâmetros: `n_estimators=500, max_depth=15, min_samples_leaf=5`

2. **XGBoost**:
   - Alta performance em dados tabulares
   - Regularização L1 e L2 para evitar overfitting
   - Tratamento eficiente de valores ausentes
   - Hiperparâmetros: `learning_rate=0.01, max_depth=8, subsample=0.8`

3. **Rede Neural Profunda**:
   - Arquitetura: 5 camadas densas com ativação ReLU
   - Dropout para regularização
   - Batch normalization para treinamento estável
   - Otimizador Adam com learning rate adaptativo

4. **Autoencoder para Detecção de Anomalias**:
   - Arquitetura codificador-decodificador simétrica
   - Treinado apenas em transações legítimas
   - Detecção baseada no erro de reconstrução
   - Complementa os modelos supervisionados

**Meta-modelo de Agregação**:
- Modelo de regressão logística que combina as previsões dos modelos base
- Pesos ajustados dinamicamente com base na performance recente
- Calibração de probabilidades usando Platt Scaling

**Estratégias para Dados Desbalanceados**:
- Undersampling da classe majoritária
- SMOTE para oversampling da classe minoritária
- Pesos de classe no treinamento
- Threshold de decisão otimizado com F1-score

</td>
<td>

### 🇺🇸 English

The system uses an ensemble of four complementary models:

1. **Random Forest**:
   - Excellent for capturing non-linear relationships
   - Robust to outliers and noisy data
   - Low risk of overfitting with cross-validation
   - Hyperparameters: `n_estimators=500, max_depth=15, min_samples_leaf=5`

2. **XGBoost**:
   - High performance on tabular data
   - L1 and L2 regularization to prevent overfitting
   - Efficient handling of missing values
   - Hyperparameters: `learning_rate=0.01, max_depth=8, subsample=0.8`

3. **Deep Neural Network**:
   - Architecture: 5 dense layers with ReLU activation
   - Dropout for regularization
   - Batch normalization for stable training
   - Adam optimizer with adaptive learning rate

4. **Autoencoder for Anomaly Detection**:
   - Symmetric encoder-decoder architecture
   - Trained only on legitimate transactions
   - Detection based on reconstruction error
   - Complements supervised models

**Aggregation Meta-model**:
- Logistic regression model that combines predictions from base models
- Weights dynamically adjusted based on recent performance
- Probability calibration using Platt Scaling

**Strategies for Imbalanced Data**:
- Undersampling of the majority class
- SMOTE for oversampling the minority class
- Class weights in training
- Decision threshold optimized with F1-score

</td>
</tr>
</table>

---

## ⚙️ Tecnologias Utilizadas | Technologies Used

<table>
<tr>
<td>

### 🇧🇷 Português

**Linguagens de Programação**:
- Python 3.9+
- SQL
- Scala (para componentes Spark)

**Frameworks e Bibliotecas**:
- **Machine Learning**: Scikit-learn, XGBoost, TensorFlow, PyTorch
- **Processamento de Dados**: Pandas, NumPy, Spark MLlib
- **Streaming**: Apache Kafka, Kafka Streams, Spark Streaming
- **API**: FastAPI, Pydantic, Swagger
- **Visualização**: Plotly, Matplotlib, Streamlit
- **Monitoramento**: Prometheus, Grafana, MLflow
- **Testes**: Pytest, Great Expectations

**Infraestrutura**:
- **Containerização**: Docker, Docker Compose
- **Orquestração**: Kubernetes, Helm
- **CI/CD**: GitHub Actions, Jenkins
- **Armazenamento**: PostgreSQL, Redis, MinIO (S3)
- **Mensageria**: Apache Kafka, RabbitMQ

**Ferramentas de MLOps**:
- **Versionamento de Modelos**: MLflow
- **Monitoramento**: Evidently AI, Prometheus
- **Feature Store**: Feast
- **Orquestração de Workflows**: Airflow

</td>
<td>

### 🇺🇸 English

**Programming Languages**:
- Python 3.9+
- SQL
- Scala (for Spark components)

**Frameworks and Libraries**:
- **Machine Learning**: Scikit-learn, XGBoost, TensorFlow, PyTorch
- **Data Processing**: Pandas, NumPy, Spark MLlib
- **Streaming**: Apache Kafka, Kafka Streams, Spark Streaming
- **API**: FastAPI, Pydantic, Swagger
- **Visualization**: Plotly, Matplotlib, Streamlit
- **Monitoring**: Prometheus, Grafana, MLflow
- **Testing**: Pytest, Great Expectations

**Infrastructure**:
- **Containerization**: Docker, Docker Compose
- **Orchestration**: Kubernetes, Helm
- **CI/CD**: GitHub Actions, Jenkins
- **Storage**: PostgreSQL, Redis, MinIO (S3)
- **Messaging**: Apache Kafka, RabbitMQ

**MLOps Tools**:
- **Model Versioning**: MLflow
- **Monitoring**: Evidently AI, Prometheus
- **Feature Store**: Feast
- **Workflow Orchestration**: Airflow

</td>
</tr>
</table>

---

## 🚀 Instalação e Configuração | Installation and Setup

<table>
<tr>
<td>

### 🇧🇷 Português

#### Pré-requisitos

- Python 3.9+
- Docker e Docker Compose
- Git

#### Instalação Local

1. Clone o repositório:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Crie e ative um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. Instale as dependências:
```bash
pip install -r requirements.txt
```

4. Configure as variáveis de ambiente:
```bash
cp .env.example .env
# Edite o arquivo .env com suas configurações
```

5. Execute os testes:
```bash
pytest
```

#### Instalação com Docker

1. Clone o repositório:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Construa e inicie os containers:
```bash
docker-compose up -d
```

3. Verifique se todos os serviços estão rodando:
```bash
docker-compose ps
```

#### Configuração do Kubernetes

Arquivos de configuração para deployment em Kubernetes estão disponíveis no diretório `k8s/`:

```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

</td>
<td>

### 🇺🇸 English

#### Prerequisites

- Python 3.9+
- Docker and Docker Compose
- Git

#### Local Installation

1. Clone the repository:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure environment variables:
```bash
cp .env.example .env
# Edit the .env file with your settings
```

5. Run tests:
```bash
pytest
```

#### Docker Installation

1. Clone the repository:
```bash
git clone https://github.com/galafis/ai-financial-fraud-detection.git
cd ai-financial-fraud-detection
```

2. Build and start containers:
```bash
docker-compose up -d
```

3. Check if all services are running:
```bash
docker-compose ps
```

#### Kubernetes Configuration

Configuration files for Kubernetes deployment are available in the `k8s/` directory:

```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secrets.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

</td>
</tr>
</table>

---

## 📊 Exemplos de Uso | Usage Examples

<table>
<tr>
<td>

### 🇧🇷 Português

#### Usando a API REST

```python
import requests
import json

# Configuração
API_URL = "http://localhost:8000/api/v1"
API_KEY = "seu_api_key"

# Autenticação
auth_response = requests.post(
    f"{API_URL}/auth/token",
    data={"api_key": API_KEY}
)
token = auth_response.json()["access_token"]
headers = {"Authorization": f"Bearer {token}"}

# Verificar uma transação
transaction = {
    "transaction_id": "tx_123456789",
    "amount": 1500.00,
    "merchant_id": "merch_123",
    "customer_id": "cust_456",
    "timestamp": "2023-06-05T14:30:00Z",
    "payment_method": "credit_card",
    "card_last_four": "1234",
    "ip_address": "192.168.1.1",
    "device_id": "dev_789",
    "location": {
        "latitude": 40.7128,
        "longitude": -74.0060
    }
}

response = requests.post(
    f"{API_URL}/predict",
    headers=headers,
    json=transaction
)

result = response.json()
print(f"Fraud Probability: {result['fraud_probability']}")
print(f"Is Fraud: {result['is_fraud']}")
print(f"Risk Level: {result['risk_level']}")
print(f"Explanation: {result['explanation']}")
```

#### Usando o Cliente Python

```python
from fraud_detection_client import FraudDetectionClient

# Inicializar cliente
client = FraudDetectionClient(
    api_url="http://localhost:8000/api/v1",
    api_key="seu_api_key"
)

# Verificar uma transação
result = client.check_transaction(
    amount=1500.00,
    merchant_id="merch_123",
    customer_id="cust_456",
    payment_method="credit_card",
    card_last_four="1234",
    ip_address="192.168.1.1",
    device_id="dev_789",
    latitude=40.7128,
    longitude=-74.0060
)

# Processar resultado
if result.is_fraud:
    print(f"ALERTA: Transação suspeita detectada! Risco: {result.risk_level}")
    print(f"Fatores de risco: {result.risk_factors}")
else:
    print("Transação aprovada.")
```

#### Usando o Dashboard

O dashboard interativo está disponível em `http://localhost:8501` após iniciar o sistema com Docker Compose.

</td>
<td>

### 🇺🇸 English

#### Using the REST API

```python
import requests
import json

# Configuration
API_URL = "http://localhost:8000/api/v1"
API_KEY = "your_api_key"

# Authentication
auth_response = requests.post(
    f"{API_URL}/auth/token",
    data={"api_key": API_KEY}
)
token = auth_response.json()["access_token"]
headers = {"Authorization": f"Bearer {token}"}

# Check a transaction
transaction = {
    "transaction_id": "tx_123456789",
    "amount": 1500.00,
    "merchant_id": "merch_123",
    "customer_id": "cust_456",
    "timestamp": "2023-06-05T14:30:00Z",
    "payment_method": "credit_card",
    "card_last_four": "1234",
    "ip_address": "192.168.1.1",
    "device_id": "dev_789",
    "location": {
        "latitude": 40.7128,
        "longitude": -74.0060
    }
}

response = requests.post(
    f"{API_URL}/predict",
    headers=headers,
    json=transaction
)

result = response.json()
print(f"Fraud Probability: {result['fraud_probability']}")
print(f"Is Fraud: {result['is_fraud']}")
print(f"Risk Level: {result['risk_level']}")
print(f"Explanation: {result['explanation']}")
```

#### Using the Python Client

```python
from fraud_detection_client import FraudDetectionClient

# Initialize client
client = FraudDetectionClient(
    api_url="http://localhost:8000/api/v1",
    api_key="your_api_key"
)

# Check a transaction
result = client.check_transaction(
    amount=1500.00,
    merchant_id="merch_123",
    customer_id="cust_456",
    payment_method="credit_card",
    card_last_four="1234",
    ip_address="192.168.1.1",
    device_id="dev_789",
    latitude=40.7128,
    longitude=-74.0060
)

# Process result
if result.is_fraud:
    print(f"ALERT: Suspicious transaction detected! Risk: {result.risk_level}")
    print(f"Risk factors: {result.risk_factors}")
else:
    print("Transaction approved.")
```

#### Using the Dashboard

The interactive dashboard is available at `http://localhost:8501` after starting the system with Docker Compose.

</td>
</tr>
</table>

---

## 📈 Resultados e Performance | Results and Performance

<table>
<tr>
<td>

### 🇧🇷 Português

O sistema foi avaliado em um conjunto de dados contendo mais de 10 milhões de transações financeiras, com aproximadamente 0.5% de transações fraudulentas.

#### Métricas de Performance

| Modelo | Precisão | Recall | F1-Score | AUC-ROC |
|--------|----------|--------|----------|---------|
| Random Forest | 0.92 | 0.85 | 0.88 | 0.96 |
| XGBoost | 0.94 | 0.87 | 0.90 | 0.97 |
| Rede Neural | 0.91 | 0.89 | 0.90 | 0.96 |
| Autoencoder | 0.88 | 0.92 | 0.90 | 0.95 |
| **Ensemble** | **0.95** | **0.91** | **0.93** | **0.98** |

#### Latência

- **Tempo médio de resposta**: 45ms
- **Percentil 95**: 78ms
- **Percentil 99**: 95ms

#### Throughput

- **Transações por segundo**: 10,000+
- **Capacidade diária**: 864+ milhões de transações

#### Economia

Baseado em uma taxa de fraude de 0.5% e um valor médio de transação fraudulenta de R$1,200, o sistema é capaz de prevenir aproximadamente R$4.6 milhões em fraudes por dia, considerando uma taxa de detecção de 91%.

#### Comparação com Sistemas Anteriores

| Métrica | Sistema Anterior | Sistema Atual | Melhoria |
|---------|-----------------|--------------|----------|
| F1-Score | 0.82 | 0.93 | +13.4% |
| Falsos Positivos | 0.15% | 0.05% | -66.7% |
| Latência Média | 250ms | 45ms | -82.0% |
| Throughput | 1,000 TPS | 10,000+ TPS | +900.0% |

</td>
<td>

### 🇺🇸 English

The system was evaluated on a dataset containing over 10 million financial transactions, with approximately 0.5% fraudulent transactions.

#### Performance Metrics

| Model | Precision | Recall | F1-Score | AUC-ROC |
|-------|-----------|--------|----------|---------|
| Random Forest | 0.92 | 0.85 | 0.88 | 0.96 |
| XGBoost | 0.94 | 0.87 | 0.90 | 0.97 |
| Neural Network | 0.91 | 0.89 | 0.90 | 0.96 |
| Autoencoder | 0.88 | 0.92 | 0.90 | 0.95 |
| **Ensemble** | **0.95** | **0.91** | **0.93** | **0.98** |

#### Latency

- **Average response time**: 45ms
- **95th percentile**: 78ms
- **99th percentile**: 95ms

#### Throughput

- **Transactions per second**: 10,000+
- **Daily capacity**: 864+ million transactions

#### Cost Savings

Based on a fraud rate of 0.5% and an average fraudulent transaction value of $240, the system can prevent approximately $920,000 in fraud per day, considering a detection rate of 91%.

#### Comparison with Previous Systems

| Metric | Previous System | Current System | Improvement |
|--------|----------------|---------------|-------------|
| F1-Score | 0.82 | 0.93 | +13.4% |
| False Positives | 0.15% | 0.05% | -66.7% |
| Average Latency | 250ms | 45ms | -82.0% |
| Throughput | 1,000 TPS | 10,000+ TPS | +900.0% |

</td>
</tr>
</table>

---

## 🔍 Explicabilidade | Explainability

<table>
<tr>
<td>

### 🇧🇷 Português

O sistema utiliza técnicas avançadas de explicabilidade para tornar as decisões dos modelos interpretáveis:

#### SHAP (SHapley Additive exPlanations)

SHAP é utilizado para explicar as previsões individuais, atribuindo a cada feature uma importância baseada na teoria dos jogos cooperativos.

```python
# Exemplo de código para gerar explicações SHAP
import shap

# Carregando o modelo e o explainer
model = load_model("models/ensemble_model.pkl")
explainer = shap.TreeExplainer(model)

# Gerando valores SHAP para uma transação
transaction_features = preprocess_transaction(transaction)
shap_values = explainer.shap_values(transaction_features)

# Visualizando as features mais importantes
shap.summary_plot(shap_values, transaction_features)
```

#### LIME (Local Interpretable Model-agnostic Explanations)

LIME é usado para criar aproximações locais interpretáveis do modelo complexo.

```python
# Exemplo de código para gerar explicações LIME
from lime import lime_tabular

# Criando o explainer
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=feature_names,
    class_names=["Legítima", "Fraudulenta"],
    mode="classification"
)

# Gerando explicação para uma transação
explanation = explainer.explain_instance(
    transaction_features,
    model.predict_proba,
    num_features=10
)

# Visualizando a explicação
explanation.show_in_notebook()
```

#### Regras de Negócio Interpretáveis

Além das técnicas algorítmicas, o sistema também extrai regras de negócio interpretáveis a partir dos modelos:

- "Transação com valor 10x maior que a média do cliente"
- "Compra em país diferente do habitual sem notificação prévia"
- "Múltiplas transações de pequeno valor em curto período"
- "Primeira compra em e-commerce de alto risco"

Estas regras são apresentadas aos analistas de fraude junto com as explicações algorítmicas para facilitar a tomada de decisão.

</td>
<td>

### 🇺🇸 English

The system uses advanced explainability techniques to make model decisions interpretable:

#### SHAP (SHapley Additive exPlanations)

SHAP is used to explain individual predictions, assigning each feature an importance based on cooperative game theory.

```python
# Example code to generate SHAP explanations
import shap

# Loading the model and explainer
model = load_model("models/ensemble_model.pkl")
explainer = shap.TreeExplainer(model)

# Generating SHAP values for a transaction
transaction_features = preprocess_transaction(transaction)
shap_values = explainer.shap_values(transaction_features)

# Visualizing the most important features
shap.summary_plot(shap_values, transaction_features)
```

#### LIME (Local Interpretable Model-agnostic Explanations)

LIME is used to create interpretable local approximations of the complex model.

```python
# Example code to generate LIME explanations
from lime import lime_tabular

# Creating the explainer
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=feature_names,
    class_names=["Legitimate", "Fraudulent"],
    mode="classification"
)

# Generating explanation for a transaction
explanation = explainer.explain_instance(
    transaction_features,
    model.predict_proba,
    num_features=10
)

# Visualizing the explanation
explanation.show_in_notebook()
```

#### Interpretable Business Rules

In addition to algorithmic techniques, the system also extracts interpretable business rules from the models:

- "Transaction with value 10x higher than customer average"
- "Purchase in a different country than usual without prior notification"
- "Multiple small-value transactions in a short period"
- "First purchase at high-risk e-commerce"

These rules are presented to fraud analysts along with algorithmic explanations to facilitate decision-making.

</td>
</tr>
</table>

---

## 🧪 Testes | Testing

<table>
<tr>
<td>

### 🇧🇷 Português

O sistema inclui uma suíte completa de testes para garantir qualidade e robustez:

#### Testes Unitários

```bash
# Executar todos os testes unitários
pytest tests/unit/

# Executar testes específicos
pytest tests/unit/test_models.py
pytest tests/unit/test_features.py
```

#### Testes de Integração

```bash
# Executar testes de integração
pytest tests/integration/

# Testar integração com Kafka
pytest tests/integration/test_kafka_integration.py
```

#### Testes de Performance

```bash
# Executar testes de performance
pytest tests/performance/

# Testar latência e throughput
pytest tests/performance/test_latency.py
```

#### Testes de Qualidade de Dados

```bash
# Executar testes de qualidade de dados
pytest tests/data_quality/

# Verificar schema e distribuições
pytest tests/data_quality/test_data_drift.py
```

#### Cobertura de Testes

A cobertura atual de testes é de 92%, com foco especial nos componentes críticos do sistema.

```bash
# Gerar relatório de cobertura
pytest --cov=src tests/
```

</td>
<td>

### 🇺🇸 English

The system includes a comprehensive test suite to ensure quality and robustness:

#### Unit Tests

```bash
# Run all unit tests
pytest tests/unit/

# Run specific tests
pytest tests/unit/test_models.py
pytest tests/unit/test_features.py
```

#### Integration Tests

```bash
# Run integration tests
pytest tests/integration/

# Test Kafka integration
pytest tests/integration/test_kafka_integration.py
```

#### Performance Tests

```bash
# Run performance tests
pytest tests/performance/

# Test latency and throughput
pytest tests/performance/test_latency.py
```

#### Data Quality Tests

```bash
# Run data quality tests
pytest tests/data_quality/

# Check schema and distributions
pytest tests/data_quality/test_data_drift.py
```

#### Test Coverage

The current test coverage is 92%, with special focus on critical system components.

```bash
# Generate coverage report
pytest --cov=src tests/
```

</td>
</tr>
</table>

---

## 📚 Documentação Adicional | Additional Documentation

<table>
<tr>
<td>

### 🇧🇷 Português

Documentação adicional está disponível no diretório `docs/`:

- [Guia de Arquitetura](docs/architecture_guide_pt.md)
- [Manual do Usuário](docs/user_manual_pt.md)
- [Guia de API](docs/api_guide_pt.md)
- [Especificação de Features](docs/feature_specification_pt.md)
- [Guia de Deployment](docs/deployment_guide_pt.md)
- [Guia de Monitoramento](docs/monitoring_guide_pt.md)
- [Estratégia de Testes](docs/testing_strategy_pt.md)

A documentação da API também está disponível em formato Swagger/OpenAPI em `http://localhost:8000/docs` quando o sistema está em execução.

</td>
<td>

### 🇺🇸 English

Additional documentation is available in the `docs/` directory:

- [Architecture Guide](docs/architecture_guide_en.md)
- [User Manual](docs/user_manual_en.md)
- [API Guide](docs/api_guide_en.md)
- [Feature Specification](docs/feature_specification_en.md)
- [Deployment Guide](docs/deployment_guide_en.md)
- [Monitoring Guide](docs/monitoring_guide_en.md)
- [Testing Strategy](docs/testing_strategy_en.md)

API documentation is also available in Swagger/OpenAPI format at `http://localhost:8000/docs` when the system is running.

</td>
</tr>
</table>

---

## 🤝 Contribuições | Contributing

<table>
<tr>
<td>

### 🇧🇷 Português

Contribuições são bem-vindas! Por favor, siga estas etapas:

1. Faça um fork do repositório
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Faça commit das suas mudanças (`git commit -m 'Adiciona nova feature'`)
4. Faça push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

Por favor, certifique-se de que seu código passa em todos os testes e segue as convenções de estilo do projeto.

</td>
<td>

### 🇺🇸 English

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature/new-feature`)
5. Open a Pull Request

Please make sure your code passes all tests and follows the project's style conventions.

</td>
</tr>
</table>

---

## 📄 Licença | License

<table>
<tr>
<td>

### 🇧🇷 Português

Este projeto está licenciado sob a licença MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

</td>
<td>

### 🇺🇸 English

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

</td>
</tr>
</table>

---

## 📞 Contato | Contact

<table>
<tr>
<td>

### 🇧🇷 Português

Gabriel Demetrios Lafis - [GitHub](https://github.com/galafis)

Link do projeto: [https://github.com/galafis/ai-financial-fraud-detection](https://github.com/galafis/ai-financial-fraud-detection)

</td>
<td>

### 🇺🇸 English

Gabriel Demetrios Lafis - [GitHub](https://github.com/galafis)

Project Link: [https://github.com/galafis/ai-financial-fraud-detection](https://github.com/galafis/ai-financial-fraud-detection)

</td>
</tr>
</table>

